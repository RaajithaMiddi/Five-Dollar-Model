{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "b26629fc2d7ca83c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-24T08:41:05.771677Z",
     "start_time": "2024-07-24T08:41:05.768502Z"
    }
   },
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import time \n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T02:24:10.035806Z",
     "start_time": "2024-07-24T02:24:10.034477Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "83fd8dbdab0162b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T02:24:10.050957Z",
     "start_time": "2024-07-24T02:24:10.036420Z"
    }
   },
   "cell_type": "code",
   "source": "data = np.load('datasets/emoji_apple_style.npy', allow_pickle=True).item()",
   "id": "c4ec8847364209fe",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T02:24:10.055458Z",
     "start_time": "2024-07-24T02:24:10.052213Z"
    }
   },
   "cell_type": "code",
   "source": "data.keys()",
   "id": "5074269279a19c18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'labels', 'embeddings', 'color_palette'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T03:06:52.079190Z",
     "start_time": "2024-07-24T03:06:52.071104Z"
    }
   },
   "cell_type": "code",
   "source": "data = list(data)",
   "id": "93671e515d0eba68",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T03:06:54.645477Z",
     "start_time": "2024-07-24T03:06:53.357770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sentence embedding model\n",
    "# https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n",
    "\n",
    "# limit of 512 word pieces, trained on length of 250 word pieces and might not work for longer texts\n",
    "\n",
    "uri_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(uri_name)\n",
    "model = AutoModel.from_pretrained(uri_name)"
   ],
   "id": "7cc3146ffeee98c2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrymei/anaconda3/envs/five-dollar-model/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Generation",
   "id": "661de64a2bb5a020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create embeddings",
   "id": "1c3664de4510a99d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:24:27.003985Z",
     "start_time": "2024-07-24T07:24:27.000645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mean Pooling - Take average of all tokens\n",
    "# see: https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1#pytorch-usage-huggingface-transformers\n",
    "def _mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ],
   "id": "7581be0f8c9081c2",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:24:28.241262Z",
     "start_time": "2024-07-24T07:24:28.237684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _tokenize(texts, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Generate token for input text. \n",
    "    :param texts: a list of input sentences or texts to be processed\n",
    "    :param tokenizer: a Hugging Face tokenizer instance\n",
    "    :param max_length: an optional parameter for padding/truncation of text strings\n",
    "    :return: encoded inputs \n",
    "    \"\"\"\n",
    "    \n",
    "    padding = True if max_length == 0 else 'max_length'\n",
    "    \n",
    "    # __call__ the tokenizer \n",
    "    return tokenizer(\n",
    "        texts, \n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_length,  # if left unset, uses model default \n",
    "        return_tensors='pt'  # return as torch tensors \n",
    "    )    "
   ],
   "id": "b6dcec421a9b6349",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:24:30.151715Z",
     "start_time": "2024-07-24T07:24:30.147790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _embed(encoded_input, model):\n",
    "    \"\"\"\n",
    "    Take tokenized values and generate embeddings\n",
    "    :param encoded_input: encoded inputs generated by tokenizer \n",
    "    :param model: a Hugging Face model instance\n",
    "    :return: embedding vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():  # only need forward pass here\n",
    "        embeddings_words = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform mean pooling\n",
    "    # The attention mask ensures that padding tokens do not contribute to the averaged embedding.\n",
    "    # purpose is to take variable length sequences and output fixed length ones \n",
    "    embeddings_sentence = _mean_pooling(embeddings_words, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings -- L2 = 1\n",
    "    embeddings_sentence = F.normalize(embeddings_sentence, p=2, dim=1)\n",
    "    \n",
    "    return embeddings_sentence, embeddings_words"
   ],
   "id": "c10954079b5c2f17",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:24:34.432454Z",
     "start_time": "2024-07-24T07:24:34.428601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_sent_word_embeddings(labels, model, tokenizer, max_length=None):\n",
    "    \"\"\"\n",
    "    Generate sentence embedding for input texts\n",
    "    :param model: Hugging Face model instance\n",
    "    :param tokenizer: Hugging Face tokenizer instance\n",
    "    :param labels: input labels from our training data \n",
    "    :param max_length: maximum length for padding/truncation fo input stirngs\n",
    "    :return: both mean-pooled sentence embedding and masked word embeddings \n",
    "    \"\"\"\n",
    "    encoded = _tokenize(labels, tokenizer, max_length)\n",
    "    embeddings_sentences, embeddings_words = _embed(encoded, model)\n",
    "\n",
    "    embeddings_words = embeddings_words['last_hidden_state'].detach().cpu().numpy()\n",
    "    embeddings_sentences = embeddings_sentences.detach().cpu().numpy()\n",
    "\n",
    "    return embeddings_sentences, embeddings_words"
   ],
   "id": "410102564bf0fe31",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Augmentation",
   "id": "78d1ecb91af42e7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gaussian Noise",
   "id": "26efed55ac407f6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_noise_to_embeddings(embeddings, num_augmentations, noise_std_dev=0.01):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param embeddings: input list of embeddings generated by get_sent_word_embeddings()\n",
    "    :param num_augmentations: number of noise variations to add\n",
    "    :param noise_std_dev: the standard deviation of multiplicative gaussian noise to add \n",
    "    :return: list of augmented embeddings and a list of their original embedding indices\n",
    "    \"\"\"\n",
    "    augmented_embeddings, augmented_idxs = [], []\n",
    "    \n",
    "    # Add Gaussian noise to the embeddings\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        for _ in range(num_augmentations):\n",
    "            # mean of 1, because this is multiplicative, since we want values that are 0 (or close) to stay 0\n",
    "            noise = np.random.normal(1, noise_std_dev, embedding.shape)\n",
    "            augmented_embedding = embedding * noise\n",
    "\n",
    "            augmented_embeddings.append(augmented_embedding)\n",
    "            augmented_idxs.append(i)\n",
    "    \n",
    "    # only return augmented values\n",
    "    return augmented_embeddings, augmented_idxs"
   ],
   "id": "9382bc75fc61e83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mixup (Random)",
   "id": "a8e2dcd2ccea0b4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T05:51:08.473303Z",
     "start_time": "2024-07-25T05:51:08.465386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mixup_aug(imgs, embeddings, n_mixups=1, lmbda=0.5):\n",
    "    \"\"\"\n",
    "    Applies mixup by randomly picking another image and soft updating the image and embedding\n",
    "    to produce a new sample. \n",
    "    \n",
    "    :param imgs: raw pre OHT images \n",
    "    :param embeddings: corresponding embedding vectors \n",
    "    :param n_mixups: number of random observations to mix up \n",
    "    :param lmbda: soft update parameter \n",
    "    :return: augmented images, embeddings, and original idxs \n",
    "    \"\"\"\n",
    "    augmented_imgs, augmented_embeddings, augmented_idxs = [], [], []\n",
    "    n_embeddings = len(embeddings)\n",
    "    \n",
    "    for i, (img, embedding) in enumerate(zip(imgs,embeddings)):\n",
    "        # Randomly select n_mixups indices without replacement\n",
    "        mixup_indices = np.random.choice(n_embeddings, n_mixups, replace=False)\n",
    "        \n",
    "        # make sure we're logging what was picked for debugging purposes \n",
    "        augmented_idxs.append([i] + mixup_indices)\n",
    "        \n",
    "        # for each sampled index... \n",
    "        for idx in mixup_indices:\n",
    "            # Get the corresponding levels, labels, and embeddings\n",
    "            mix_img = imgs[idx]\n",
    "            mix_embedding = embeddings[idx]\n",
    "            \n",
    "            # Interpolate the levels, labels, and embeddings\n",
    "            augmented_img = lmbda * img + (1 - lmbda) * mix_img\n",
    "            augmented_embedding = lmbda * embedding + (1 - lmbda) * mix_embedding\n",
    "            \n",
    "            # Append the new data to the augmented lists\n",
    "            augmented_imgs.append(augmented_img)\n",
    "            augmented_embeddings.append(augmented_embedding)\n",
    "\n",
    "    # only return the augmented values\n",
    "    return augmented_imgs, augmented_embeddings, augmented_idxs"
   ],
   "id": "4c7edb3ffc37df56",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GPT",
   "id": "dd20fa73c358de96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load API key",
   "id": "ccb8a08973972ee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 117,
   "source": [
    "with open('apikey.env', 'r') as file:\n",
    "    # Read the content of the file\n",
    "    api_key = file.read().strip() "
   ],
   "id": "19c2887b3c2556c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 118,
   "source": "client = OpenAI(api_key=api_key)",
   "id": "60ff3a1bb4a858e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Estimate API token usage",
   "id": "85a6d537f3fa19ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 119,
   "source": [
    "def _compute_tokens_from_payload(payload, encoding):\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens required in the request. \n",
    "    See: https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "    \n",
    "    :param payload: an array of dicts or an array of strings \n",
    "    :param encoding: a tiktoken encoder instance \n",
    "    :return: a count of tokens \n",
    "    \"\"\"\n",
    "    num_tokens = 0\n",
    "    for message in payload:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        \n",
    "        # if we pass a wellformed payload \n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "                \n",
    "            # if there's a name, the role is not processed and so doesn't count to api usage\n",
    "            if key == \"name\":  \n",
    "                num_tokens += -1 \n",
    "    \n",
    "    # pad the estimate with the structure of response; note: does not include actual response\n",
    "    # every reply is primed with <im_start>assistant     \n",
    "    num_tokens += 2  \n",
    "    \n",
    "    return num_tokens"
   ],
   "id": "5dc83887bb3a795d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 120,
   "source": [
    "def _compute_tokens_from_list(labels, encoding):\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens required in the request. \n",
    "    See: https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "    \n",
    "    :param labels: a list of string labels \n",
    "    :param encoding: a tiktoken encoder instance \n",
    "    :return: a count of tokens \n",
    "    \"\"\"\n",
    "    return [len(encoding.encode(l)) for l in labels]\n"
   ],
   "id": "eed9c9008289b4cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### API calls and packaging",
   "id": "73352065b8c6785c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 121,
   "source": [
    "def _create_payload(labels):\n",
    "    prompt = \"\"\"Take each string in the list provided, and write an alternate label for each one. These strings describe an image of a pixel video game map. These alternate labels should describe the same image as the original label, but use different words and a different sentence structure. Use simple or common words when writing the alternate labels. Assume you have the vocabulary of a 10 year old. Your output should have the same number of strings as the input list.\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with excellent attention to detail. You only output python lists of strings according to the instructions you are given. Output the list on a single line, without any newlines. Make sure every list is closed properly\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt} Here is the list of labels: {labels}\"},            \n",
    "    ]"
   ],
   "id": "dd2c6783f39a1000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 122,
   "source": [
    "def _call_gpt(messages, model):\n",
    "    return client.chat.completions.create(model=model, messages=messages)\n",
    "    # return openai.ChatCompletion.create(model=model, messages=messages)"
   ],
   "id": "1da52371195fb2c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 123,
   "source": [
    "# segment the data into sublists to not exceed api limits\n",
    "\n",
    "# tiktoken is a fast open-source tokenizer by OpenAI.\n",
    "\n",
    "# Given a text string (e.g., \"tiktoken is great!\") and an encoding (e.g., \"cl100k_base\"), a tokenizer can split the text string into a list of tokens (e.g., [\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]).\n",
    "# \n",
    "# Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token).\n",
    "def _chunk_labels(labels, encoding, threshold):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param labels: feature inputs as a list of strings\n",
    "    :param encoding: a tiktoken encoder instance\n",
    "    :param threshold: a maximum token size to chunk the inputs into \n",
    "    :return: \n",
    "    \"\"\"\n",
    "\n",
    "    current_chunk, chunks = [], []\n",
    "    count_tokens = 0\n",
    "    label_tokens = _compute_tokens_from_list(labels, encoding)\n",
    "\n",
    "    # step throw all labels \n",
    "    for label, tokens in zip(labels, label_tokens):\n",
    "        print(label)\n",
    "        print(tokens)\n",
    "        # append labels to the current chunk and update our count\n",
    "        current_chunk.append(label)\n",
    "        count_tokens += tokens \n",
    "        \n",
    "        # if the array exceeds the token threshold then.... \n",
    "        if count_tokens + 2 > threshold:\n",
    "            \n",
    "            # remove the last label \n",
    "            hold = current_chunk.pop()\n",
    "            \n",
    "            # complain if the label itself is so big that it's as big as the threshold\n",
    "            if tokens > threshold:\n",
    "                raise Exception(f\"Label {label} is too big: *{tokens} tokens* for this threshold: *{threshold} tokens*\")\n",
    "            \n",
    "            # since we removed the offending label, the current chunk should be the right size\n",
    "            chunks.append(current_chunk)\n",
    "            \n",
    "            # start a new arr with the one we popped out and reset our counter\n",
    "            current_chunk = [hold] \n",
    "            count_tokens = 0\n",
    "            \n",
    "    # add the final set of labels \n",
    "    chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks"
   ],
   "id": "dfafabdb44cf5dd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a buff man with a blue headband and red shirt\n",
      "11\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Label a buff man with a blue headband and red shirt is too big: *11 tokens* for this threshold: *1 tokens*",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpt-4o-mini\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      9\u001B[0m encoding \u001B[38;5;241m=\u001B[39m tiktoken\u001B[38;5;241m.\u001B[39mencoding_for_model(model)\n\u001B[0;32m---> 11\u001B[0m \u001B[43m_chunk_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnewunseen_sprite\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[68], line 37\u001B[0m, in \u001B[0;36m_chunk_labels\u001B[0;34m(labels, encoding, threshold)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# complain if the label itself is so big that it's as big as the threshold\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokens \u001B[38;5;241m>\u001B[39m threshold:\n\u001B[0;32m---> 37\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLabel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is too big: *\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokens\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokens* for this threshold: *\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mthreshold\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokens*\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# since we removed the offending label, the current chunk should be the right size\u001B[39;00m\n\u001B[1;32m     40\u001B[0m chunks\u001B[38;5;241m.\u001B[39mappend(current_chunk)\n",
      "\u001B[0;31mException\u001B[0m: Label a buff man with a blue headband and red shirt is too big: *11 tokens* for this threshold: *1 tokens*"
     ]
    }
   ],
   "execution_count": 71,
   "source": [
    "# newunseen_sprite = [\n",
    "#     \"a buff man with a blue headband and red shirt\", \n",
    "#     \"a blue duck with a red headband\", \n",
    "#     \"a green woman with blonde hair\", \n",
    "#     \"a dog with a black hat\", \n",
    "#     \"a man with blue shoes, a red hat, and a green shirt\"\n",
    "# ]\n",
    "# model = 'gpt-4o-mini'\n",
    "# encoding = tiktoken.encoding_for_model(model)\n",
    "# \n",
    "# _chunk_labels(newunseen_sprite, encoding, 1)"
   ],
   "id": "86cd5b9a08755d9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 124,
   "source": [
    "def _process_result(result):\n",
    "    \"\"\"\n",
    "    Filter out patterns that look like [' and '] \n",
    "    TODO: why don't we just look at those patterns directly instead of regex? \n",
    "    :param result: raw openAI API response \n",
    "    :return: processed answers \n",
    "    \"\"\"\n",
    "    answer = result.choices[0].message.content                \n",
    "    apostrophe_pattern = r\"(?<=\\w)'(?=[^,\\]])|'(?=\\w+?'\\s)\"\n",
    "    answer = re.sub(apostrophe_pattern, '', answer)\n",
    "    \n",
    "    idx_open = answer.find(\"[\")\n",
    "    idx_close = answer.find(']') + 1 # +1 since indexing ignores current spot \n",
    "    \n",
    "    return ast.literal_eval(answer[idx_open:idx_close])"
   ],
   "id": "9a5c6fc0cc4d534a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Overall function",
   "id": "6adad05730ec10cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 125,
   "source": [
    "def get_gpt_alt_labels(labels, model='gpt-4o-mini', num_retries=3, debug=True):\n",
    "    \"\"\"\n",
    "    Call GPT to generate alternate labels. \n",
    "    :param labels: list of human-annotated labels \n",
    "    :param num_retries: how many times do we try again? \n",
    "    :return: (a list of alternate labels, api call response status)\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # it's probably clk100k_base (can pass to get_encoding()), but let's not assume\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    prompt_size = _compute_tokens_from_payload(_create_payload([]), encoding)\n",
    "    \n",
    "    # chunk the prompt to the right size; if we pass in everything, it's gonna timeout/fail\n",
    "    threshold = 4000 - prompt_size \n",
    "    chunks = _chunk_labels(labels, encoding, threshold=threshold)\n",
    "\n",
    "    \n",
    "    if debug:\n",
    "        print(f'Prompt size: {prompt_size}')\n",
    "        print(f\"split time = {time.time() - start}\")\n",
    "        print(\"Number of loops: \", len(chunks))\n",
    "        \n",
    "    alt_labels = []\n",
    "    \n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        tries = 0\n",
    "        success = False\n",
    "        start = time.time()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Loop {i} running through array of size {len(labels)}\")\n",
    "\n",
    "        while not success and tries < num_retries:\n",
    "            payload = _create_payload(chunk)\n",
    "            result = _call_gpt(payload, model)\n",
    "            alt_chunk = _process_result(result)\n",
    "\n",
    "            n_labels = len(chunk)\n",
    "            n_alts = len(alt_chunk)\n",
    "            \n",
    "            if n_labels == n_alts:\n",
    "                success = True\n",
    "            else:\n",
    "                tries += 1\n",
    "                \n",
    "                print(f'FAILED. {n_labels} labels but {n_alts} alts!')\n",
    "                print(f'attempting retry # {tries}')\n",
    "                \n",
    "        if success:\n",
    "            alt_labels += alt_chunk\n",
    "        else:\n",
    "            print(f\"failed completely after {num_retries} retries.\")\n",
    "            return\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"api call time = {time.time() - start}\")\n",
    "\n",
    "    return alt_labels"
   ],
   "id": "8f815b6d9c3c7137"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPT Label Mixup",
   "id": "4eb9c164546d01e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T06:20:46.107917Z",
     "start_time": "2024-07-25T06:20:46.097992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# interpolate n times between a label and its altlabel (MUST BE CALLED RIGHT AFTER GPT AUG)\n",
    "def altlabel_interp_aug(embeddings, alt_embeddings, n_steps=1):\n",
    "    \"\"\"\n",
    "    A different form of mixup where we take the original embedding and the GPT embedding and \n",
    "    generate additional embeddings in between via interpolation. \n",
    "    \n",
    "    The original method randomly picked a label but that's probably not useful here? \n",
    "    \n",
    "    :param embeddings: list of original embeddings \n",
    "    :param alt_embeddings: list of GPT embeddings\n",
    "    :param n_steps: number interpolated samples to draw \n",
    "    :return: list of interpolated embeddings and list of original indices \n",
    "    \"\"\"\n",
    "    interpolated_embeddings, interpolated_idxs = [], []\n",
    "    n_embeddings = len(embeddings) \n",
    "    \n",
    "    for i in range(n_embeddings):\n",
    "        alpha_values = np.linspace(0, 1, n_steps + 2)[1:-1]  # Exclude the 0 and 1 values\n",
    "        \n",
    "        for alpha in alpha_values:\n",
    "            interpolated_embedding = alpha * embeddings[i] + (1 - alpha) * alt_embeddings[i] \n",
    "            interpolated_embeddings.append(interpolated_embedding)\n",
    "            interpolated_idxs.append(i)\n",
    "\n",
    "    # only return the augmented samples\n",
    "    return interpolated_embeddings, interpolated_idxs"
   ],
   "id": "401099e3b6e9dcd9",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export",
   "id": "c4570e199220effe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T05:20:06.899755Z",
     "start_time": "2024-07-25T05:20:06.893273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def export_data(inputs, labels, embeddings, filename):\n",
    "    data = {\n",
    "        'annotation_ids': ann_ids,\n",
    "        'images': maps,\n",
    "        'labels': labels,\n",
    "        'embeddings': embeddings\n",
    "    }\n",
    "    np.save(filename, data, allow_pickle=True)"
   ],
   "id": "58c6ef7d9a106c7b",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test GPT Augmentation",
   "id": "6696cd2b841bd8cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a buff man with a blue headband and red shirt\n",
      "11\n",
      "a blue duck with a red headband\n",
      "8\n",
      "a green woman with blonde hair\n",
      "6\n",
      "a dog with a black hat\n",
      "6\n",
      "a man with blue shoes, a red hat, and a green shirt\n",
      "14\n",
      "Prompt size: 155\n",
      "split time = 0.0015180110931396484\n",
      "Number of loops:  1\n",
      "Loop 0 running through array of size 5\n",
      "api call time = 1.4173822402954102\n"
     ]
    }
   ],
   "execution_count": 126,
   "source": [
    "newunseen_sprite = [\n",
    "    \"a buff man with a blue headband and red shirt\", \n",
    "    \"a blue duck with a red headband\", \n",
    "    \"a green woman with blonde hair\", \n",
    "    \"a dog with a black hat\", \n",
    "    \"a man with blue shoes, a red hat, and a green shirt\"\n",
    "]\n",
    "\n",
    "# # it's probably clk100k_base (can pass to get_encoding()), but let's not assume\n",
    "# encoding = tiktoken.encoding_for_model(model)\n",
    "# prompt_size = _compute_tokens_from_payload(_create_payload([]), encoding)\n",
    "#     \n",
    "# # chunk the prompt to the right size; if we pass in everything, it's gonna timeout/fail\n",
    "# threshold = 4000 - prompt_size \n",
    "# chunks = _chunk_labels(newunseen_sprite, encoding, threshold=threshold)\n",
    "\n",
    "alt_labels = get_gpt_alt_labels(newunseen_sprite, model='gpt-4o-mini', num_retries=3, debug=True)\n"
   ],
   "id": "d64b101255517b28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a buff man with a blue headband and red shirt',\n",
       "  'a strong guy wearing a blue headband and a red top'),\n",
       " ('a blue duck with a red headband',\n",
       "  'a blue duck wearing a red band around its head'),\n",
       " ('a green woman with blonde hair', 'a lady in green with yellow hair'),\n",
       " ('a dog with a black hat', 'a dog wearing a dark hat'),\n",
       " ('a man with blue shoes, a red hat, and a green shirt',\n",
       "  'a man dressed in blue sneakers, a red cap, and a green top')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 128,
   "source": "list(zip(newunseen_sprite, alt_labels))",
   "id": "245bd5cbee722c94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 129,
   "source": "np.save('datasets/hmei_temp_test.npy', data, allow_pickle=True)",
   "id": "d5cdc1f731ad7a8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['a strong guy wearing a blue headband and a red shirt', 'a blue-colored duck wearing a red band', 'a woman in green with yellow hair', 'a dog wearing a black cap', 'a guy in blue shoes, a red cap, and a green top']\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 100,
   "source": "",
   "id": "7c17fa6adea18d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a strong guy wearing a blue headband and a red shirt', 'a blue-colored duck wearing a red band', 'a woman in green with yellow hair', 'a dog wearing a black cap', 'a guy in blue shoes, a red cap, and a green top']\n",
      "['a strong guy wearing a blue headband and a red shirt', 'a blue-colored duck wearing a red band', 'a woman in green with yellow hair', 'a dog wearing a black cap', 'a guy in blue shoes, a red cap, and a green top']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a strong guy wearing a blue headband and a red shirt',\n",
       " 'a blue-colored duck wearing a red band',\n",
       " 'a woman in green with yellow hair',\n",
       " 'a dog wearing a black cap',\n",
       " 'a guy in blue shoes, a red cap, and a green top']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 99,
   "source": "",
   "id": "239ea2bdb8f5896a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9oRgixcXuduXyurvX1BgLO80eu0rF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"['a strong guy wearing a blue headband and a red shirt', 'a blue-colored duck wearing a red band', 'a woman in green with yellow hair', 'a dog wearing a black cap', 'a guy in blue shoes, a red cap, and a green top']\", role='assistant', function_call=None, tool_calls=None))], created=1721809860, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_661538dc1f', usage=CompletionUsage(completion_tokens=57, prompt_tokens=208, total_tokens=265))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85,
   "source": "",
   "id": "6510264680170bec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenize Input",
   "id": "46d306163e0aa31c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:24:51.232513Z",
     "start_time": "2024-07-24T07:24:49.971953Z"
    }
   },
   "cell_type": "code",
   "source": "get_sent_word_embeddings(['I am a big foo bar'], model, tokenizer, None)",
   "id": "2de10334b059c0a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 9.82737169e-02,  1.92254707e-02,  1.50002148e-02,\n",
       "          2.88651371e-03, -5.50648011e-02, -3.68904248e-02,\n",
       "          1.18874013e-01,  4.83325757e-02,  6.01447141e-03,\n",
       "          4.21652161e-02, -3.52831781e-02, -8.36424306e-02,\n",
       "         -3.50537300e-02, -1.95263885e-02,  2.79278532e-02,\n",
       "          3.95715386e-02, -1.28606902e-02,  1.15014147e-02,\n",
       "         -1.06786359e-02, -2.24973243e-02, -1.10274829e-01,\n",
       "          6.65320083e-02,  1.88118499e-02,  1.76326197e-03,\n",
       "         -1.19415283e-01, -1.26929749e-02,  4.31086402e-03,\n",
       "          6.83599897e-03,  8.61172006e-02, -5.21420985e-02,\n",
       "          1.39685320e-02,  7.56450966e-02,  4.12384309e-02,\n",
       "         -1.83217507e-02, -8.99099484e-02, -2.86612986e-03,\n",
       "          4.25359234e-02, -9.02812183e-03,  3.88544612e-02,\n",
       "          4.52078208e-02,  2.26972681e-02, -9.90414154e-03,\n",
       "          3.50649208e-02,  3.34182046e-02, -3.00184805e-02,\n",
       "          1.63097437e-02, -2.29557529e-02,  4.05627266e-02,\n",
       "          7.01322453e-04, -1.12383990e-02,  1.38886692e-03,\n",
       "          7.42447227e-02, -1.00007011e-02,  8.28382149e-02,\n",
       "          1.67382304e-02, -2.78839911e-03, -6.78040907e-02,\n",
       "         -3.44755389e-02,  1.92196947e-02, -3.48526612e-03,\n",
       "          4.29658853e-02,  7.80364648e-02,  4.48678210e-02,\n",
       "          7.02279562e-04,  8.20145458e-02, -2.23266683e-03,\n",
       "          2.70966440e-02,  6.52374923e-02, -6.40463596e-03,\n",
       "          7.98105747e-02,  3.72867398e-02,  4.24769614e-03,\n",
       "         -6.62486106e-02,  7.75601864e-02,  4.17716168e-02,\n",
       "         -7.72168711e-02,  1.01112686e-02,  1.80764236e-02,\n",
       "          7.96335414e-02,  5.86113743e-02, -8.96291211e-02,\n",
       "         -5.06389104e-02, -1.38380798e-03, -6.64793104e-02,\n",
       "         -2.46749097e-03, -3.96805489e-03,  9.97465197e-03,\n",
       "          3.76925990e-02, -4.16932628e-02, -1.92991048e-02,\n",
       "         -9.22479853e-02,  2.64159404e-03, -1.92608777e-02,\n",
       "          3.44808586e-02,  6.14040717e-02, -2.68678479e-02,\n",
       "         -6.77874079e-03, -4.27825637e-02, -4.84716780e-02,\n",
       "          1.23498634e-01,  1.07441723e-01,  4.81154844e-02,\n",
       "          1.32901222e-02,  5.57614379e-02,  4.75342944e-02,\n",
       "         -4.13533412e-02, -2.51554344e-02,  1.12781353e-01,\n",
       "          8.39166865e-02, -4.27664518e-02, -8.53110105e-03,\n",
       "         -3.36282142e-02,  5.26387570e-03, -4.38221265e-03,\n",
       "         -6.17694259e-02, -4.61186729e-02, -8.85777699e-04,\n",
       "          5.69922179e-02, -1.04936454e-02, -3.08804531e-02,\n",
       "         -1.34154903e-02,  1.20151587e-01, -7.96550959e-02,\n",
       "          4.31427471e-02, -9.28648934e-02,  6.83101499e-03,\n",
       "         -4.77832705e-02,  7.37573993e-31, -1.15647636e-01,\n",
       "         -1.96219748e-03,  4.49713655e-02,  6.57176301e-02,\n",
       "          3.82031128e-02, -3.88328284e-02, -7.94089120e-03,\n",
       "          1.69904418e-02, -4.21175472e-02,  7.49837561e-03,\n",
       "          6.03163652e-02, -1.14956191e-02, -7.22538456e-02,\n",
       "          3.63481522e-04,  1.23942465e-01,  6.65539457e-03,\n",
       "         -9.39449668e-03, -3.51815969e-02, -6.88051879e-02,\n",
       "         -3.62145677e-02, -9.02091246e-03, -3.29322778e-02,\n",
       "          4.11546864e-02,  9.85549856e-03,  2.23536845e-02,\n",
       "         -3.80400233e-02,  4.50804830e-02, -3.67927887e-02,\n",
       "         -1.85810570e-02,  3.73182446e-02, -4.90485765e-02,\n",
       "          3.58422683e-03,  3.99393635e-03, -5.49102612e-02,\n",
       "          1.86835579e-03, -1.30513772e-01, -5.72470482e-04,\n",
       "         -1.53122740e-02,  8.82203430e-02, -5.35000255e-03,\n",
       "         -5.20937778e-02, -1.95381418e-02, -4.19064872e-02,\n",
       "         -3.31136249e-02, -1.25052612e-02,  6.04682490e-02,\n",
       "          3.74963395e-02, -4.42484654e-02, -1.41351029e-01,\n",
       "         -2.55801585e-02,  2.71049589e-02,  1.13575859e-03,\n",
       "          5.36750965e-02, -3.54647450e-03, -4.11116285e-03,\n",
       "         -3.15940008e-02,  5.03812321e-02,  5.28129451e-02,\n",
       "          1.40596610e-02,  5.06689027e-02, -5.90926334e-02,\n",
       "          3.59506928e-03, -9.54693928e-03,  8.85809138e-02,\n",
       "         -1.21140681e-01, -6.22852333e-02,  1.87307335e-02,\n",
       "         -5.37946336e-02,  3.97551022e-02,  4.77865934e-02,\n",
       "          7.55114257e-02,  4.20161225e-02, -1.11647854e-02,\n",
       "          1.02780294e-02, -3.20102386e-02,  1.60439219e-02,\n",
       "          7.69499806e-04,  1.26448115e-02, -1.02818094e-01,\n",
       "          7.15114847e-02,  6.69072717e-02,  1.35030067e-02,\n",
       "          8.19593668e-04, -2.77979076e-02,  3.28418501e-02,\n",
       "         -7.58768320e-02,  1.17611839e-02, -8.57803226e-02,\n",
       "          3.76714468e-02,  8.63641966e-03, -1.40528977e-01,\n",
       "          4.78797182e-02,  1.46232359e-02, -7.83592910e-02,\n",
       "         -4.45432961e-02, -2.40803746e-33,  4.16097268e-02,\n",
       "          1.63817834e-02,  3.88807170e-02, -1.83814503e-02,\n",
       "         -1.74494479e-02, -7.57726803e-02,  2.64125522e-02,\n",
       "          1.36527151e-01, -8.80574286e-02,  2.16953997e-02,\n",
       "         -5.91607653e-02,  7.35736871e-03,  2.57605277e-02,\n",
       "         -4.84430864e-02,  8.28052461e-02, -4.76319529e-03,\n",
       "          2.92176567e-02,  1.54949632e-02, -6.97620809e-02,\n",
       "          4.29453030e-02, -3.02874893e-02, -1.90106742e-02,\n",
       "          3.91713157e-02,  9.08220652e-03, -8.29778910e-02,\n",
       "         -3.32784243e-02,  3.67563851e-02,  2.16448791e-02,\n",
       "         -7.48910010e-02, -4.57959548e-02, -1.75534617e-02,\n",
       "         -6.87493710e-03, -8.24583173e-02, -2.85593811e-02,\n",
       "          2.80280020e-02, -6.16848655e-02, -1.80274006e-02,\n",
       "          6.67102030e-03, -9.63613112e-03,  4.07471247e-02,\n",
       "         -3.31874117e-02,  1.95336472e-02,  3.01342569e-02,\n",
       "          6.55118795e-03,  1.40230646e-02, -5.91010377e-02,\n",
       "          7.35435111e-04, -6.91907331e-02,  1.68860238e-02,\n",
       "          2.29332875e-02, -2.79153977e-02, -1.85902771e-02,\n",
       "          4.30558398e-02, -5.91536658e-03,  2.49888953e-02,\n",
       "         -4.40363139e-02, -4.05406859e-03,  4.89950478e-02,\n",
       "         -7.95468017e-02, -1.00296892e-01, -7.41371140e-02,\n",
       "          3.73657160e-02, -1.03026750e-02,  1.95257254e-02,\n",
       "          5.91366477e-02, -5.16587943e-02, -7.89476484e-02,\n",
       "          5.41422106e-02, -7.59624168e-02,  2.44150241e-03,\n",
       "         -2.92918533e-02, -2.55327821e-02, -1.30963743e-01,\n",
       "          1.19338758e-01, -7.03503862e-02, -4.86700125e-02,\n",
       "          1.90375447e-02, -1.99970845e-02,  6.71506301e-02,\n",
       "          6.70660883e-02,  4.54936549e-02, -1.43012824e-02,\n",
       "          1.02839500e-01, -6.14209548e-02, -1.89853832e-02,\n",
       "          6.51976019e-02,  1.32523235e-02,  9.86507982e-02,\n",
       "         -3.75788920e-02,  4.61206846e-02,  1.95370074e-02,\n",
       "          6.10832423e-02, -1.03131264e-01, -5.82588129e-02,\n",
       "          3.38412225e-02, -2.91124235e-33, -6.68746084e-02,\n",
       "         -3.36944242e-03,  2.95035783e-02,  7.70974383e-02,\n",
       "          2.21637655e-02, -2.74540717e-03, -1.06040858e-01,\n",
       "         -1.03981756e-02, -5.25747389e-02,  3.51665579e-02,\n",
       "         -3.08401082e-02,  3.61353308e-02,  1.50847817e-02,\n",
       "          2.48017516e-02, -1.07229371e-02, -5.75151294e-02,\n",
       "         -1.91334728e-02, -6.24795631e-02, -1.17316991e-02,\n",
       "         -2.71596666e-03, -5.23565598e-02,  3.99304517e-02,\n",
       "          3.52488644e-02,  6.13601469e-02, -5.85577786e-02,\n",
       "         -8.76521319e-02,  1.93715934e-02,  2.97862552e-02,\n",
       "         -5.03605604e-03,  3.19483653e-02,  4.72950190e-02,\n",
       "          9.39064100e-02, -1.10841967e-01, -2.47408319e-02,\n",
       "          4.22241539e-02, -5.52880019e-02, -2.10137535e-02,\n",
       "         -2.61025708e-02, -2.85804737e-02,  1.65574308e-02,\n",
       "          1.38622094e-02, -2.02616286e-02,  6.25886172e-02,\n",
       "          4.88931686e-02, -4.48889472e-02,  8.59383866e-03,\n",
       "          1.81503650e-02,  6.42469600e-02,  1.45664327e-02,\n",
       "         -2.28815544e-02,  8.46575722e-02, -1.85921658e-02,\n",
       "          3.11373975e-02,  8.60187039e-02,  2.69187745e-02,\n",
       "          6.18502535e-02, -4.06395495e-02,  3.54668088e-02,\n",
       "         -7.99907744e-02, -2.85273716e-02,  5.10216691e-02,\n",
       "         -2.52831494e-03, -6.20616376e-02, -6.09108061e-02]], dtype=float32),\n",
       " array([[[ 3.55168998e-01,  2.92323828e-02,  1.01136915e-01, ...,\n",
       "          -2.64515188e-02, -4.78748590e-01, -2.42160782e-02],\n",
       "         [ 9.94790614e-01, -7.65286386e-01,  2.22761825e-01, ...,\n",
       "          -2.15924010e-01, -1.14791393e+00, -1.77797949e+00],\n",
       "         [ 8.99299562e-01,  3.04892778e-01,  1.99052259e-01, ...,\n",
       "           1.53585626e-02, -1.07575513e-01, -1.10056889e+00],\n",
       "         ...,\n",
       "         [ 2.86401689e-01,  3.16010684e-01,  1.78617984e-01, ...,\n",
       "          -1.05946243e-01, -6.41173542e-01, -2.44580239e-01],\n",
       "         [ 3.39628875e-01,  3.23380798e-01,  1.77090153e-01, ...,\n",
       "           3.80171463e-04, -8.17297757e-01, -3.15862387e-01],\n",
       "         [ 1.83073953e-01,  2.41282403e-01,  1.34854168e-01, ...,\n",
       "          -7.20330551e-02, -6.22304976e-01, -1.61387235e-01]]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # # load data, filter it, and generate the embeddings\n",
    "    # ann_ids, maps, annotations, authors = get_db_data(cursor=mycursor)\n",
    "    # print(\"Number of levels in db: \", len(ann_ids))\n",
    "    # \n",
    "    # ann_ids, maps, annotations = sort_by_annid(ann_ids, maps, annotations)\n",
    "    # sent_embeddings, word_embeddings = get_sent_word_embeddings(model, tokenizer, annotations, max_len=25)\n",
    "    # data = {\n",
    "    #         'ann_ids' : ann_ids,\n",
    "    #         'images': maps,\n",
    "    #         'labels': annotations,\n",
    "    #         'embeddings': sent_embeddings,\n",
    "    #         'word_embeddings': word_embeddings,\n",
    "    #     }\n",
    "    # np.save('datasets/Map Data/maps_noaug.npy', data, allow_pickle=True)"
   ],
   "id": "21149a39ca7a128"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# do gpt alt label augmentation\n",
    "    ann_ids, maps, annotations, embeddings, authors = gpt_augmentation(ann_ids, maps, annotations, sent_embeddings, authors, model)\n",
    "    print(\"Number of levels before filtering: \", len(ann_ids))\n",
    "\n",
    "    ann_ids_exp = np.array(ann_ids)\n",
    "    maps_exp = np.array(maps)\n",
    "    annotations_exp = np.array(annotations)\n",
    "    embeddings_exp = np.array(embeddings)\n",
    "\n",
    "\n",
    "    export_data(ann_ids_exp, maps_exp, annotations_exp, embeddings_exp, 'datasets/maps_gpt4_aug.npy')"
   ],
   "id": "df7d3536e51258eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e1f7958f71b30ed9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
